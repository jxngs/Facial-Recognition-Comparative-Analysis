{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from tensorflow.keras import layers\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "\n",
    "face_dataset = sklearn.datasets.fetch_lfw_people(min_faces_per_person=10)\n",
    "IMAGE_SIZE = face_dataset.images[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4324,)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "face_dataset.target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = tf.data.Dataset.from_tensor_slices((np.reshape(face_dataset.images, (-1, IMAGE_SIZE[0], IMAGE_SIZE[1], 1)), face_dataset.target))\n",
    "batch_size = 64\n",
    "ds = ds.shuffle(len(face_dataset.images), seed=1).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_SIZE = int(len(face_dataset.images) / batch_size * 0.8)\n",
    "train_ds = ds.take(TRAIN_SIZE)\n",
    "test_ds = ds.skip(TRAIN_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Xception model from https://keras.io/examples/vision/image_classification_from_scratch/#using-image-data-augmentation\n",
    "\n",
    "def make_model(input_shape, num_classes):\n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "\n",
    "    # Entry block\n",
    "    x = layers.Rescaling(1.0 / 255)(inputs)\n",
    "    x = layers.Conv2D(128, 3, strides=2, padding=\"same\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(\"relu\")(x)\n",
    "\n",
    "    previous_block_activation = x  # Set aside residual\n",
    "\n",
    "    for size in [256, 512, 728]:\n",
    "        x = layers.Activation(\"relu\")(x)\n",
    "        x = layers.SeparableConv2D(size, 3, padding=\"same\")(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "\n",
    "        x = layers.Activation(\"relu\")(x)\n",
    "        x = layers.SeparableConv2D(size, 3, padding=\"same\")(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "\n",
    "        x = layers.MaxPooling2D(3, strides=2, padding=\"same\")(x)\n",
    "\n",
    "        # Project residual\n",
    "        residual = layers.Conv2D(size, 1, strides=2, padding=\"same\")(\n",
    "            previous_block_activation\n",
    "        )\n",
    "        x = layers.add([x, residual])  # Add back residual\n",
    "        previous_block_activation = x  # Set aside next residual\n",
    "\n",
    "    x = layers.SeparableConv2D(1024, 3, padding=\"same\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(\"relu\")(x)\n",
    "\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    if num_classes == 2:\n",
    "        activation = \"sigmoid\"\n",
    "        units = 1\n",
    "    else:\n",
    "        activation = \"softmax\"\n",
    "        units = num_classes\n",
    "\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    outputs = layers.Dense(units, activation=activation)(x)\n",
    "    return keras.Model(inputs, outputs)\n",
    "\n",
    "\n",
    "model = make_model(input_shape=IMAGE_SIZE + (1, ), num_classes=face_dataset.target_names.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 62, 47, 1)\n"
     ]
    }
   ],
   "source": [
    "for imgs, labels in test_ds.take(1):\n",
    "    print(imgs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54/54 [==============================] - 62s 1s/step - loss: 4.4701 - accuracy: 0.1400 - val_loss: 5.0095 - val_accuracy: 0.1348\n",
      "Epoch 2/25\n",
      "54/54 [==============================] - 67s 1s/step - loss: 3.4353 - accuracy: 0.2766 - val_loss: 5.0256 - val_accuracy: 0.0081\n",
      "Epoch 3/25\n",
      "54/54 [==============================] - 69s 1s/step - loss: 2.2077 - accuracy: 0.4789 - val_loss: 5.0645 - val_accuracy: 0.0058\n",
      "Epoch 4/25\n",
      "54/54 [==============================] - 66s 1s/step - loss: 1.3268 - accuracy: 0.6736 - val_loss: 5.1042 - val_accuracy: 0.0104\n",
      "Epoch 5/25\n",
      "54/54 [==============================] - 65s 1s/step - loss: 0.7898 - accuracy: 0.8061 - val_loss: 5.3085 - val_accuracy: 0.0138\n",
      "Epoch 6/25\n",
      "54/54 [==============================] - 67s 1s/step - loss: 0.4557 - accuracy: 0.8961 - val_loss: 5.5168 - val_accuracy: 0.0092\n",
      "Epoch 7/25\n",
      "54/54 [==============================] - 70s 1s/step - loss: 0.2512 - accuracy: 0.9453 - val_loss: 6.0816 - val_accuracy: 0.0058\n",
      "Epoch 8/25\n",
      "54/54 [==============================] - 63s 1s/step - loss: 0.1461 - accuracy: 0.9751 - val_loss: 7.2223 - val_accuracy: 0.0058\n",
      "Epoch 9/25\n",
      "54/54 [==============================] - 54s 1s/step - loss: 0.0683 - accuracy: 0.9919 - val_loss: 8.2573 - val_accuracy: 0.0035\n",
      "Epoch 10/25\n",
      "54/54 [==============================] - 53s 991ms/step - loss: 0.0386 - accuracy: 0.9974 - val_loss: 9.7438 - val_accuracy: 0.0046\n",
      "Epoch 11/25\n",
      "54/54 [==============================] - 52s 973ms/step - loss: 0.0198 - accuracy: 0.9991 - val_loss: 10.3090 - val_accuracy: 0.0104\n",
      "Epoch 12/25\n",
      "54/54 [==============================] - 52s 971ms/step - loss: 0.0121 - accuracy: 0.9994 - val_loss: 11.1043 - val_accuracy: 0.0138\n",
      "Epoch 13/25\n",
      "54/54 [==============================] - 52s 973ms/step - loss: 0.0083 - accuracy: 1.0000 - val_loss: 10.0763 - val_accuracy: 0.0127\n",
      "Epoch 14/25\n",
      "54/54 [==============================] - 53s 987ms/step - loss: 0.0056 - accuracy: 1.0000 - val_loss: 5.6138 - val_accuracy: 0.1636\n",
      "Epoch 15/25\n",
      "54/54 [==============================] - 53s 988ms/step - loss: 0.0047 - accuracy: 1.0000 - val_loss: 2.9756 - val_accuracy: 0.3387\n",
      "Epoch 16/25\n",
      "54/54 [==============================] - 54s 998ms/step - loss: 0.0034 - accuracy: 1.0000 - val_loss: 2.0530 - val_accuracy: 0.4896\n",
      "Epoch 17/25\n",
      "54/54 [==============================] - 221s 4s/step - loss: 0.0031 - accuracy: 1.0000 - val_loss: 0.1817 - val_accuracy: 0.9574\n",
      "Epoch 18/25\n",
      "54/54 [==============================] - 662s 12s/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 0.0300 - val_accuracy: 0.9977\n",
      "Epoch 19/25\n",
      "54/54 [==============================] - 100s 2s/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 0.0382 - val_accuracy: 0.9954\n",
      "Epoch 20/25\n",
      "54/54 [==============================] - 74s 1s/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.0159 - val_accuracy: 0.9988\n",
      "Epoch 21/25\n",
      "54/54 [==============================] - 76s 1s/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.0015 - val_accuracy: 1.0000\n",
      "Epoch 22/25\n",
      "54/54 [==============================] - 79s 1s/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.0015 - val_accuracy: 1.0000\n",
      "Epoch 23/25\n",
      "54/54 [==============================] - 74s 1s/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.0082 - val_accuracy: 1.0000\n",
      "Epoch 24/25\n",
      "54/54 [==============================] - 81s 1s/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.0016 - val_accuracy: 1.0000\n",
      "Epoch 25/25\n",
      "54/54 [==============================] - 76s 1s/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 5.7878e-04 - val_accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x3235dc970>"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 25\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\"save_at_{epoch}.keras\"),\n",
    "]\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.legacy.Adam(1e-3), # legacy for m1 support\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "model.fit(\n",
    "    train_ds,\n",
    "    epochs=epochs,\n",
    "    callbacks=callbacks,\n",
    "    validation_data=test_ds,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

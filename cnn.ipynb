{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from tensorflow.keras import layers\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "face_dataset = datasets.fetch_lfw_people(min_faces_per_person=50)\n",
    "IMAGE_SIZE = face_dataset.images[0].shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/wAALCAA+AC8BAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/AOfsHMlyij7iH5R7+tdtZzi2AQlACBgkgVnaukcoYBGGTnoSP0rDsLJoXeQqSCeAOB+JNaNxIrIEXaQP7qM2PxPFclrcTxaokkRzkEccg1c0KX/SJJCvyAdfeuiTXoIEEV1Jtdh8gAzVS4maXLQpEU65jPP5VViuQXVEtzvH94H+ZJqW5vRCrSM4nnxhVY7gn0FYl6jXMli5BXcXDKOxxW7pOnLa21zExDGOU7WH8Q7VjXcwM7BYAZGOTI5wFFULbVVimbayrg4yhyDWrNqj/ZTM6lYiOWC9ayYZ7y9usxLshwWDD+vrW9JMNMs4bu4hWUq5OwnAJIxV2wugjTSRzLNEH2l0GFyRnAqW+hikhZrQZkccr3NYg8PSB1ku4EhXrgAAmuqayt5PDU0AhBjC+n3q5mzsEVAYZSU7A/yqh4nu3lt7e3J48wkEegFb2gwq3w8nnAH7nWP3nsGTGarpcPbSEqenrUD6hcXlymV3xo4Zg3RgD0rTk8URSvKqwGAAn90eg+lZSzN5rSpGEjnw2xT90/8A16w9dkP26CFgQFjMmT3ycV1nw4u4bka14du2xHqNvvi9pE7j3qm6S2lzJZ3oxPHwx7MP7w+tUpbiePK20auF5wTis2Sed7kZsJTLn+9lT+Na9hBdvJFHKVMkjBVRBwKX4nWa6bDod3ABtSJ7Z898HOf1rlrS/m02+t7y2crPBIHQ+/p+PSvTJ5rPxJptvfxgDfyrL9+F+6n2rGurUwKBIiq3/PRejVnZnLlYod3PXJ4rrPDmmyeaLuSMhY+VJHVvajx7p/27wxGm0Fo58j8a8pkNbXg/V3sdYFm4L21220r/AHW7EV6FNAoZkYBhnoRmuL8W3BhubXT7ctF5gMsjIcZA4AFaXhPUL22+0It1I0aKqrG53KDnrg10F7qL61pk0PlrDNCyV//Z",
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=47x62>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Show the first image in the image dataset\n",
    "first_image = face_dataset.images[0]\n",
    "img = keras.preprocessing.image.array_to_img(np.expand_dims(first_image * 255, axis=-1))\n",
    "display(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out images if they're the > 150th image for a class -- don't want too many of some classes\n",
    "kept_images = []\n",
    "kept_labels = []\n",
    "frequencies = {}\n",
    "\n",
    "for image, label in zip(face_dataset.images, face_dataset.target):\n",
    "    if label not in frequencies:\n",
    "        frequencies[label] = 0\n",
    "    if frequencies[label] > 150:\n",
    "        continue\n",
    "    kept_images.append(image)\n",
    "    kept_labels.append(label)\n",
    "    frequencies[label] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remap labels from [0, N - 1]\n",
    "\n",
    "old_labels_to_new_labels = {}\n",
    "for label in kept_labels:\n",
    "    if label not in old_labels_to_new_labels:\n",
    "        old_labels_to_new_labels[label] = len(old_labels_to_new_labels)\n",
    "\n",
    "kept_labels = [old_labels_to_new_labels[label] for label in kept_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = np.expand_dims(np.asarray(kept_images), axis=-1) # necessary to show there is 1 channel (grayscale)?\n",
    "labels = keras.utils.to_categorical(np.asarray(kept_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images, val_images, train_labels, val_labels = train_test_split(images, labels, test_size=0.2, random_state=42)\n",
    "datagen = keras.preprocessing.image.ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    horizontal_flip=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen.fit(train_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_generator = datagen.flow(train_images, train_labels, batch_size=64, shuffle=True)\n",
    "val_data_generator = datagen.flow(val_images, val_labels, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/wAALCAA+AC8BAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/AKCstjo9xHZqA8zgM2OQtaGlWwZQ5G0Zrs7OFGfJxheABWnhfLCjvVK6gaNCfWufulyxGea5VEkjtkg3KzFucV0NlLaWiKbiUBemT0JrqLKJGiWSNgwPOQasqz5O1eRVe4lkbhgAc1hXikSnFYFjYG41qdfMUwJGjKp64YZHNaVhprzX8sWpSqthsIUBM4btWjokostQfTw6tEqlx2wKTUPEtxZhpYAqw5+9IuR+lWrfUJry2Wd2hdT18qqVwd0rY7Vj+G5mnuZHkBDOqqARyNoxiu2ltY7a2M0qjAXODWRaRlUuL10VWuOOnaootEQ2EcJbfFnlTzV86fHHJD5X7tkGBg8Y9KpXtvcQzuWuotrcgCPkfrWX4QVrmGK9m7pvYjuTya3b25e+JTnH8Iodr94oLcwReVH94k8mtbSMIhhnAIYkqPT2ovJ7eKUiNfmFcdqtzNcXR2N06mpfCAWLQLdXOQbdf5VKZrpb/ehZo/RVBxWgNWwhM0JJ/wBkYJ/A1PBdLcKHh3ZDAjcMEUl86p50jHnGBXH3VwC4VCOOtWbeO50C2tNPvc7hCoEgHDDFbtkPNdXAxx271oSKNwHAI5psdxHCzNsGf61z2v6iYUKjLSOMgCvI9f8AFF/aaoYYCAiqM8dTX0d470y3fSbSTGHRtgIHY1x2m6lLaxgbQ207etW59YZ0LlDuPFZrarOvzkKcdBUFpaSatqcUUkgEly4Xd2UVyXj3w1aW3i68ggAVI9r61//Z",
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=47x62>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# display first image in val data batch and its label -- just for validation\n",
    "first_batch = next(val_data_generator)\n",
    "display(keras.preprocessing.image.array_to_img(first_batch[0][0] * 255))\n",
    "print(first_batch[1][0])\n",
    "\n",
    "val_data_generator = datagen.flow(val_images, val_labels, batch_size=64, shuffle=True) # reset val_data_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Xception model from https://keras.io/examples/vision/image_classification_from_scratch/#using-image-data-augmentation\n",
    "\n",
    "# def make_model(input_shape, num_classes):\n",
    "#     inputs = keras.Input(shape=input_shape)\n",
    "\n",
    "#     # Entry block\n",
    "#     x = layers.Rescaling(1.0 / 255)(inputs)\n",
    "#     x = layers.Conv2D(128, 3, strides=2, padding=\"same\")(x)\n",
    "#     x = layers.BatchNormalization()(x)\n",
    "#     x = layers.Activation(\"relu\")(x)\n",
    "\n",
    "#     previous_block_activation = x  # Set aside residual\n",
    "\n",
    "#     for size in [256, 512, 728]:\n",
    "#         x = layers.Activation(\"relu\")(x)\n",
    "#         x = layers.SeparableConv2D(size, 3, padding=\"same\")(x)\n",
    "#         x = layers.BatchNormalization()(x)\n",
    "\n",
    "#         x = layers.Activation(\"relu\")(x)\n",
    "#         x = layers.SeparableConv2D(size, 3, padding=\"same\")(x)\n",
    "#         x = layers.BatchNormalization()(x)\n",
    "\n",
    "#         x = layers.MaxPooling2D(3, strides=2, padding=\"same\")(x)\n",
    "\n",
    "#         # Project residual\n",
    "#         residual = layers.Conv2D(size, 1, strides=2, padding=\"same\")(\n",
    "#             previous_block_activation\n",
    "#         )\n",
    "#         x = layers.add([x, residual])  # Add back residual\n",
    "#         previous_block_activation = x  # Set aside next residual\n",
    "\n",
    "#     x = layers.SeparableConv2D(1024, 3, padding=\"same\")(x)\n",
    "#     x = layers.BatchNormalization()(x)\n",
    "#     x = layers.Activation(\"relu\")(x)\n",
    "\n",
    "#     x = layers.GlobalAveragePooling2D()(x)\n",
    "#     if num_classes == 2:\n",
    "#         activation = \"sigmoid\"\n",
    "#         units = 1\n",
    "#     else:\n",
    "#         activation = \"softmax\"\n",
    "#         units = num_classes\n",
    "\n",
    "#     x = layers.Dropout(0.5)(x)\n",
    "#     outputs = layers.Dense(units, activation=activation)(x)\n",
    "#     return keras.Model(inputs, outputs)\n",
    "\n",
    "\n",
    "# model = make_model(input_shape=IMAGE_SIZE + (1, ), num_classes=len(frequencies))\n",
    "\n",
    "input_shape = IMAGE_SIZE + (1, )\n",
    "num_classes = len(frequencies)\n",
    "\n",
    "model = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=input_shape),\n",
    "        layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Flatten(),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(num_classes, activation=\"softmax\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 12)"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(train_data_generator)[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "14/14 [==============================] - 1s 54ms/step - loss: 1.6479 - accuracy: 0.4680 - val_loss: 1.6503 - val_accuracy: 0.4864\n",
      "Epoch 2/25\n",
      "14/14 [==============================] - 1s 46ms/step - loss: 1.5537 - accuracy: 0.5057 - val_loss: 1.5308 - val_accuracy: 0.4818\n",
      "Epoch 3/25\n",
      "14/14 [==============================] - 1s 43ms/step - loss: 1.5335 - accuracy: 0.4932 - val_loss: 1.4799 - val_accuracy: 0.5455\n",
      "Epoch 4/25\n",
      "14/14 [==============================] - 1s 43ms/step - loss: 1.4358 - accuracy: 0.5342 - val_loss: 1.4244 - val_accuracy: 0.5727\n",
      "Epoch 5/25\n",
      "14/14 [==============================] - 1s 44ms/step - loss: 1.3984 - accuracy: 0.5365 - val_loss: 1.4506 - val_accuracy: 0.5545\n",
      "Epoch 6/25\n",
      "14/14 [==============================] - 1s 44ms/step - loss: 1.3628 - accuracy: 0.5468 - val_loss: 1.3801 - val_accuracy: 0.5318\n",
      "Epoch 7/25\n",
      "14/14 [==============================] - 1s 45ms/step - loss: 1.3399 - accuracy: 0.5468 - val_loss: 1.3330 - val_accuracy: 0.5773\n",
      "Epoch 8/25\n",
      "14/14 [==============================] - 1s 43ms/step - loss: 1.2704 - accuracy: 0.5947 - val_loss: 1.3033 - val_accuracy: 0.5682\n",
      "Epoch 9/25\n",
      "14/14 [==============================] - 1s 47ms/step - loss: 1.2396 - accuracy: 0.5970 - val_loss: 1.2055 - val_accuracy: 0.6273\n",
      "Epoch 10/25\n",
      "14/14 [==============================] - 1s 43ms/step - loss: 1.2080 - accuracy: 0.6096 - val_loss: 1.2717 - val_accuracy: 0.5864\n",
      "Epoch 11/25\n",
      "14/14 [==============================] - 1s 44ms/step - loss: 1.2345 - accuracy: 0.6233 - val_loss: 1.2362 - val_accuracy: 0.6091\n",
      "Epoch 12/25\n",
      "14/14 [==============================] - 1s 45ms/step - loss: 1.1424 - accuracy: 0.6324 - val_loss: 1.1871 - val_accuracy: 0.6227\n",
      "Epoch 13/25\n",
      "14/14 [==============================] - 1s 45ms/step - loss: 1.1183 - accuracy: 0.6518 - val_loss: 1.2534 - val_accuracy: 0.6000\n",
      "Epoch 14/25\n",
      "14/14 [==============================] - 1s 44ms/step - loss: 1.0847 - accuracy: 0.6598 - val_loss: 1.1114 - val_accuracy: 0.6818\n",
      "Epoch 15/25\n",
      "14/14 [==============================] - 1s 44ms/step - loss: 1.0666 - accuracy: 0.6610 - val_loss: 1.2267 - val_accuracy: 0.5909\n",
      "Epoch 16/25\n",
      "14/14 [==============================] - 1s 44ms/step - loss: 1.0442 - accuracy: 0.6724 - val_loss: 1.1243 - val_accuracy: 0.5955\n",
      "Epoch 17/25\n",
      "14/14 [==============================] - 1s 45ms/step - loss: 1.0419 - accuracy: 0.6678 - val_loss: 1.0544 - val_accuracy: 0.6636\n",
      "Epoch 18/25\n",
      "14/14 [==============================] - 1s 47ms/step - loss: 1.0326 - accuracy: 0.6655 - val_loss: 1.0983 - val_accuracy: 0.6682\n",
      "Epoch 19/25\n",
      "14/14 [==============================] - 1s 44ms/step - loss: 0.9672 - accuracy: 0.6906 - val_loss: 1.0764 - val_accuracy: 0.6636\n",
      "Epoch 20/25\n",
      "14/14 [==============================] - 1s 44ms/step - loss: 0.9900 - accuracy: 0.6849 - val_loss: 1.0785 - val_accuracy: 0.6727\n",
      "Epoch 21/25\n",
      "14/14 [==============================] - 1s 44ms/step - loss: 0.9806 - accuracy: 0.7158 - val_loss: 1.0430 - val_accuracy: 0.6545\n",
      "Epoch 22/25\n",
      "14/14 [==============================] - 1s 45ms/step - loss: 0.9549 - accuracy: 0.7055 - val_loss: 1.0446 - val_accuracy: 0.6955\n",
      "Epoch 23/25\n",
      "14/14 [==============================] - 1s 52ms/step - loss: 0.9495 - accuracy: 0.6929 - val_loss: 1.0465 - val_accuracy: 0.7000\n",
      "Epoch 24/25\n",
      "14/14 [==============================] - 1s 45ms/step - loss: 0.9171 - accuracy: 0.7009 - val_loss: 1.0322 - val_accuracy: 0.7227\n",
      "Epoch 25/25\n",
      "14/14 [==============================] - 1s 44ms/step - loss: 0.8994 - accuracy: 0.7237 - val_loss: 1.0203 - val_accuracy: 0.7045\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x2934aeac0>"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 25\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\"save_at_{epoch}.keras\"),\n",
    "]\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.legacy.Adam(1e-3), # legacy for m1 support\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "model.fit(\n",
    "    train_data_generator,\n",
    "    epochs=epochs,\n",
    "    callbacks=callbacks,\n",
    "    validation_data=val_data_generator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

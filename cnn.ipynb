{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from tensorflow.keras import layers\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "face_dataset = datasets.fetch_lfw_people(min_faces_per_person=50)\n",
    "IMAGE_SIZE = face_dataset.images[0].shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/wAALCAA+AC8BAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/AOfsHMlyij7iH5R7+tdtZzi2AQlACBgkgVnaukcoYBGGTnoSP0rDsLJoXeQqSCeAOB+JNaNxIrIEXaQP7qM2PxPFclrcTxaokkRzkEccg1c0KX/SJJCvyAdfeuiTXoIEEV1Jtdh8gAzVS4maXLQpEU65jPP5VViuQXVEtzvH94H+ZJqW5vRCrSM4nnxhVY7gn0FYl6jXMli5BXcXDKOxxW7pOnLa21zExDGOU7WH8Q7VjXcwM7BYAZGOTI5wFFULbVVimbayrg4yhyDWrNqj/ZTM6lYiOWC9ayYZ7y9usxLshwWDD+vrW9JMNMs4bu4hWUq5OwnAJIxV2wugjTSRzLNEH2l0GFyRnAqW+hikhZrQZkccr3NYg8PSB1ku4EhXrgAAmuqayt5PDU0AhBjC+n3q5mzsEVAYZSU7A/yqh4nu3lt7e3J48wkEegFb2gwq3w8nnAH7nWP3nsGTGarpcPbSEqenrUD6hcXlymV3xo4Zg3RgD0rTk8URSvKqwGAAn90eg+lZSzN5rSpGEjnw2xT90/8A16w9dkP26CFgQFjMmT3ycV1nw4u4bka14du2xHqNvvi9pE7j3qm6S2lzJZ3oxPHwx7MP7w+tUpbiePK20auF5wTis2Sed7kZsJTLn+9lT+Na9hBdvJFHKVMkjBVRBwKX4nWa6bDod3ABtSJ7Z898HOf1rlrS/m02+t7y2crPBIHQ+/p+PSvTJ5rPxJptvfxgDfyrL9+F+6n2rGurUwKBIiq3/PRejVnZnLlYod3PXJ4rrPDmmyeaLuSMhY+VJHVvajx7p/27wxGm0Fo58j8a8pkNbXg/V3sdYFm4L21220r/AHW7EV6FNAoZkYBhnoRmuL8W3BhubXT7ctF5gMsjIcZA4AFaXhPUL22+0It1I0aKqrG53KDnrg10F7qL61pk0PlrDNCyV//Z",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAC8AAAA+CAAAAACexh1OAAAIeElEQVR4AR1WWY8jVxW+y7lLbXbZbXfPdPfMZBMIgUgQBJI3XhBv/F6EEERKhHggoLAkhDCZyfRMJr3YLtu13P1ymkeXq+495zvfcuhvxHK12Z6dWrWUWso87Mfk7/ZkONBmVmmZocy7nsjkXn5H5xokZ/2R8FxWIFUJnqVImLaWSkJI8lTkrE5q431WcU9LkHLaxoZZGoaspkB1y52VFYnJR0JFWSjuGA2OCVmP/Q5omkylUiZUMQuU5wzZJEoZTbKYKUWN2TlGmEhCFd4CsY4x/Fuy6SB1UYRApQ7ZcinbGd1boeQJIS5mRzN+BdHhgV5x/FgAJeOYjU+s4nKiOnZ+eV6HaToMoqoHrfsIzhNKE+Mh8qSJZCQaMvXB4fUh1k01Xu08pUpJywvtNZBEkihkcCUZOstqreogC+5MlkLx/hWBMym8TduJVbe0AZdySCxjH+3FyN14FQikIPNItTNMNA8r293uZWlMKWOIkGjAeoHTHE0QEsp4TNG44R7rupThJe/v2Pkb7tVkoxlGwIITEYzG7dQoMjgGYux7Wp1JnINxUh468cajw5VRiTiiwEByjvDUGT06HJOZHMl6vNsKljlQb6d6qZ6+HkWtDZEl2Jycp2w0bFNeVCVcb4aQZVVSIXIYJynWM781yXT2YSE9AOB8IPQm4WSfj1xXj01vqO8dYZly0FIRl8IwhDE25YGBN7ouso05zncvnW4bUc9L0tsYbJJ6Xgt3IGaXaiw5CMbxeNE2hCBfrl8eSGCD3EFVzqR3JpGqdOaw55EWtgAXZ7cKMuGVDkSx8c5qQISOVHtfqMJKSgGr6D0wrnBmQayv5pAI1IomTg1ZqUAC8j0ObqrqgkUex31UVdz35Vxkzsv5CsoByUFS8qls5TBFoBEHF51r3SjChMLaboc8HFclce70MVw8TVLS6C0phDkg+DhsRpzBhidITPubm6jncpxETaF5Bm/dRCZ4dMax6+0RmeSinpVI2Sn6TOl001G/ZyvBoYQ4/h7OzzaZM+pGE6cjWZ1Od7sjkonISLOT434A0pH6JEgpIVoD9jS5TBIelZMom6E7RoSckbFsuXXXB1+0F+pMByVQhj7Da9oSy1OFA06Vcvtv92xpJ4HQLnTav3BoCE0zc1nVkuXewTAVyQMKJE261QfhUT4xZ17WJ2u1ux10JWlCpqmShdgHWF91XAhJKz5aGpN6cub8FIKer05WIN7aD1PNc2DZB+5DF2GbrZuoSEqqwQfbMy48p8VyvUTq5Afv3ExG33MyDzQPlsDXYYboDFKBVvujYCNhMTenRa1yYom2pZ0i55STY1RHUPDhK5MiOhmAYr13nI9EVSdtEtyoqiA+1w06R+Qk2rT3Fczh3+MsTjlMBVONcXlWlmiPkluLEyLRUcEUoNI5xxES+JS9ONdHOibpKdGrSi4rPwyMedQ0zQwO8YQGtEkbqXdYxhfH/OvmtQgeiUBkdVprv48idnrdlmBkc71Rs5AEyT5HB3jALr1/eYh06tGcOWex6/poXo8zmcLhiJ2Tb46QzBQiJYRHIKquQiDcxwA0hPEYEOq74YMf3XTMI50f039smhDQdFCQyN0MKmOKCHuPUnZAvEv+lp6uOxMVYT29fDWOBEUEFBXK0JKVoDQAmnQOKE5iTMZ0uF5aOQycHlPB+23LsefYW0yniHZxovcnaCf3l8bggh2p+vKK1iROLF9rNe9TAzmTo2sIULDT26fbHAMhGQty3k95UcWv94t5Rdjxbn1ebAxoSuw2UYIImbF6+JmBidBEEANr2UlFZiIdp3lND6IkvDEhKZg6rjwQ7u7Im2yzxtBA2eAcy6ZM3ENLkvU86zxgiiDHm7Y50MAo89/4dy66QAmTgjMiFnOGSUrQBdxoOQaWR4XTYfaT08FQliB8eXP5Q7vF6XMhlZAioeqDMRFNJSstGOKeEyyfzN0gmKfw9FnzbnGFwREIw/z242AGT3jdNuWsiNvrDbpSni1hDQcPAeTmy5++9e5vdwt+P3GMg/uJCYnQSZG/Q2zOEX1x0dJHRcchR9l9fnX6/uc3Al2HAVZh+2HwHIM3J39M7fphHsPiPI2XZ90E91H31XO++uXvNqHGe0kKuVTl4NBUWODN6oFGGyFNNZkTZDbygRTPvnhkvr//ZAfKEwyJpIrkyLQ5RF2ftoqiXdTnMJC2FQlyyvX1Rx9WL793/HO3kD4QNDbcJoQuDJWluqdtpou1n3w5L3rm8Q39t0+qYH/8XrgNdc1xSQiZBYvKLRhGJEKlF2gRljw4Rc9K0Ynw0YtF53/xofrO183JsshoAAQ3oTBZxonUbRuzInm1LlnOSNXiv5+Qeuff/Pn8roO2LQUKjVHQWnLsh/J2HhPaTFVroIzHXHQfv7na38jVe8832C1Hp0roLRA811xnvpIucay71mgCuHDQ+tkfp5U7jNXbl7nrLEoI6+xvr3eGYwxXK9yxSES4apYQjGgL/qe/r1bZj+ziDbH3FFlDcZ8rqgpYrC6qjHXg0GcNoDQJ8aPefHx5Zvcssbne3fUcH7ICGVhIUj2+oI4yJC20+D6qivIg58//8KtiT3BbUWf07qC4iFJjDJDm/AE7cETn/4GCegzoWrjlkX+dvVebwImFFWxS4rhpmuEwntfL2czTlAi2VAJyDLMoJlDbvywfs+N9o6Kd25h6Ef/6LC7sM/nBB6CENd46wH0SS00Z58Cef7aY+xDQ+1DWiAN8e+WW8zRdL9ZBnK5Oshl6wOc0xfuFUw//ufhZ06OnRNRyCu34xfGyyFLpE78z3y7OH1cc90+GLoK9oFWp7tPZDwpUK56AP+HVc0x+IBOvfRL2q6cXj54cGV6O0GETKUv56rNtUXAM+Pvrjv88LEDXuK1xk4Er96Jr1/8DKxFNsNYgbJcAAAAASUVORK5CYII=",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=47x62>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Show the first image in the image dataset\n",
    "first_image = face_dataset.images[0]\n",
    "img = keras.preprocessing.image.array_to_img(np.expand_dims(first_image * 255, axis=-1))\n",
    "display(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out images if they're the > 150th image for a class -- don't want too many of some classes\n",
    "kept_images = []\n",
    "kept_labels = []\n",
    "frequencies = {}\n",
    "\n",
    "for image, label in zip(face_dataset.images, face_dataset.target):\n",
    "    if label not in frequencies:\n",
    "        frequencies[label] = 0\n",
    "    if frequencies[label] > 150:\n",
    "        continue\n",
    "    kept_images.append(image)\n",
    "    kept_labels.append(label)\n",
    "    frequencies[label] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remap labels from [0, N - 1]\n",
    "\n",
    "old_labels_to_new_labels = {}\n",
    "for label in kept_labels:\n",
    "    if label not in old_labels_to_new_labels:\n",
    "        old_labels_to_new_labels[label] = len(old_labels_to_new_labels)\n",
    "\n",
    "kept_labels = [old_labels_to_new_labels[label] for label in kept_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = np.expand_dims(np.asarray(kept_images), axis=-1) # necessary to show there is 1 channel (grayscale)?\n",
    "labels = np.asarray(kept_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images, val_images, train_labels, val_labels = train_test_split(images, labels, test_size=0.2, random_state=42)\n",
    "datagen = keras.preprocessing.image.ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    horizontal_flip=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen.fit(train_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_generator = datagen.flow(train_images, train_labels, batch_size=64, shuffle=True)\n",
    "val_data_generator = datagen.flow(val_images, val_labels, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/wAALCAA+AC8BAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/AMHRrEW1usancT3robeEJOofkVsLaxHBwB+lTPbRxREsvHY9jXPaxFGAGjArj7+ESSYdQSK6nRLXZbh5G5PNaD3FuHAaRVwfvE4rQsL+0uZDFBcGVh6jg1qSQJIzoIhkLnJ6CuZ1aS1WQxLdISvJHTmuP1NPMUPG2cnqK6u3iYWO5egXp6VmXUdjBGtxdWtzc5O0AcJn3rQga5sLmJUsWiBPzrjhR9a0ta1a4twsdihnkkT5iv8AAPeuTvLeO6nEV/bXJuGQurR9APXFZUVo0Ns+1naPd8pcV6Jo08XmGJx8rnn2rWKRwqwWMyxA8BMZFY960jq8rhowTgKTk/jWOBJazw3AbI6EmugVpLiHH2dPMIxvB7Vy+vyQWvlwg5A5c+9aEELJcBd2AehFb2mk7Gd/lIHzHNY2oz3t7JJNDAPIX5YgTjPuazdQ1YNZi0+yhbsfkKt2F/J9hkgmP7yMcEdxXN6wMLHJIc7hk/0rp7SdJJRhstnirs1wy6fM6g/f28dxWWmr3MEhinspdo4CgdKpX+orFuZNOnkkbqRjAqGwFxdZlMZj+Qk59Kx9ZuDIyx/3VHFbv9k6voM8Et+oUSMcYPat21mErRpIyhA5Y57ntWnc6WbvDo2zP8XrWbqGhCFAXmdvqaw7txZ/KG2gIRj615xqWuuurzDaHjHGPSvpvxzoq6l4clnVgklqd4PqO4rxtWk+0BhIwZOnPFXZvF19aQKG+bHQiq1z4vu7m33lckeprEs7qXVdRQ3DHDyBSB2FcbrsQt9evtf/2Q==",
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=47x62>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n"
     ]
    }
   ],
   "source": [
    "# display first image in val data batch and its label -- just for validation\n",
    "first_batch = next(val_data_generator)\n",
    "display(keras.preprocessing.image.array_to_img(first_batch[0][0] * 255))\n",
    "print(first_batch[1][0])\n",
    "\n",
    "val_data_generator = datagen.flow(val_images, val_labels, batch_size=64, shuffle=True) # reset val_data_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Xception model from https://keras.io/examples/vision/image_classification_from_scratch/#using-image-data-augmentation\n",
    "\n",
    "def make_model(input_shape, num_classes):\n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "\n",
    "    # Entry block\n",
    "    x = layers.Rescaling(1.0 / 255)(inputs)\n",
    "    x = layers.Conv2D(128, 3, strides=2, padding=\"same\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(\"relu\")(x)\n",
    "\n",
    "    previous_block_activation = x  # Set aside residual\n",
    "\n",
    "    for size in [256, 512, 728]:\n",
    "        x = layers.Activation(\"relu\")(x)\n",
    "        x = layers.SeparableConv2D(size, 3, padding=\"same\")(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "\n",
    "        x = layers.Activation(\"relu\")(x)\n",
    "        x = layers.SeparableConv2D(size, 3, padding=\"same\")(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "\n",
    "        x = layers.MaxPooling2D(3, strides=2, padding=\"same\")(x)\n",
    "\n",
    "        # Project residual\n",
    "        residual = layers.Conv2D(size, 1, strides=2, padding=\"same\")(\n",
    "            previous_block_activation\n",
    "        )\n",
    "        x = layers.add([x, residual])  # Add back residual\n",
    "        previous_block_activation = x  # Set aside next residual\n",
    "\n",
    "    x = layers.SeparableConv2D(1024, 3, padding=\"same\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(\"relu\")(x)\n",
    "\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    if num_classes == 2:\n",
    "        activation = \"sigmoid\"\n",
    "        units = 1\n",
    "    else:\n",
    "        activation = \"softmax\"\n",
    "        units = num_classes\n",
    "\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    outputs = layers.Dense(units, activation=activation)(x)\n",
    "    return keras.Model(inputs, outputs)\n",
    "\n",
    "\n",
    "model = make_model(input_shape=IMAGE_SIZE + (1, ), num_classes=len(frequencies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "14/14 [==============================] - 10s 649ms/step - loss: 2.3501 - accuracy: 0.1632 - val_loss: 2.4677 - val_accuracy: 0.1318\n",
      "Epoch 2/10\n",
      "14/14 [==============================] - 8s 598ms/step - loss: 2.0732 - accuracy: 0.2797 - val_loss: 2.4673 - val_accuracy: 0.1500\n",
      "Epoch 3/10\n",
      "14/14 [==============================] - 9s 611ms/step - loss: 1.5942 - accuracy: 0.4566 - val_loss: 2.4750 - val_accuracy: 0.1500\n",
      "Epoch 4/10\n",
      "14/14 [==============================] - 9s 664ms/step - loss: 1.1788 - accuracy: 0.6016 - val_loss: 2.4781 - val_accuracy: 0.1500\n",
      "Epoch 5/10\n",
      "14/14 [==============================] - 9s 623ms/step - loss: 0.8698 - accuracy: 0.7123 - val_loss: 2.4781 - val_accuracy: 0.1500\n",
      "Epoch 6/10\n",
      "14/14 [==============================] - 9s 623ms/step - loss: 0.5981 - accuracy: 0.7808 - val_loss: 2.4814 - val_accuracy: 0.1500\n",
      "Epoch 7/10\n",
      "14/14 [==============================] - 9s 679ms/step - loss: 0.4963 - accuracy: 0.8436 - val_loss: 2.4875 - val_accuracy: 0.1500\n",
      "Epoch 8/10\n",
      " 9/14 [==================>...........] - ETA: 3s - loss: 0.3594 - accuracy: 0.8698"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\"save_at_{epoch}.keras\"),\n",
    "]\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.legacy.Adam(1e-3), # legacy for m1 support\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "model.fit(\n",
    "    train_data_generator,\n",
    "    epochs=epochs,\n",
    "    callbacks=callbacks,\n",
    "    validation_data=val_data_generator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

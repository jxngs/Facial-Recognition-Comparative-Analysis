import numpy as np
from sklearn import datasets
from sklearn.metrics import accuracy_score
from sklearn.svm import SVC
import cv2
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.metrics import ConfusionMatrixDisplay
from sklearn.metrics import confusion_matrix
import keras
from tensorflow.keras import layers
import sys
from PyQt5.QtWidgets import QApplication, QWidget, QLabel, QComboBox, QPushButton, QVBoxLayout,QLineEdit,QFormLayout,QMessageBox, QGridLayout
from heapq import *
import random
import os
import io
from PIL import Image

class FacialRecognitionAlgorithm:
    def __init__(self, modelname, dataset):
        self.modelname = modelname
        self.dataset=dataset

    def train(self):
        pass

    def predict(self):
        pass

    def show_results(self):
        pass

class Eigenfaces(FacialRecognitionAlgorithm):

    class PrincipalComponentsAnalysis:
        def __init__(self, n_components):
            self.n_components = n_components
            self.mean = None
            self.components = None
            self.explained_variances = None

        def fit(self, X):
            self.mean = np.mean(X, axis=0)
            centered_data = X - self.mean
            covariance_matrix = np.cov(centered_data, rowvar=False)
            eigendata = np.linalg.eigh(covariance_matrix)
            eigenvalues = eigendata[0]
            eigenvectors = eigendata[1]
            sorted_indices = np.argsort(eigenvalues)[::-1]
            eigenvectors = eigenvectors[:, sorted_indices]
            self.components = eigenvectors[:, :self.n_components]
            total_variance = np.sum(eigenvalues)
            self.explained_variances = eigenvalues[sorted_indices][:self.n_components] / total_variance

        def transform(self, X):
            centered_data = X - self.mean
            projected_data = np.dot(centered_data, self.components)
            whitened_data = projected_data / np.sqrt(self.explained_variances)
            return whitened_data


    def __init__(self, modelname, dataset, num_features,c_value):
        self.n_components=num_features
        self.C=c_value
        super().__init__(modelname, dataset)
        
    def train(self):
        self.X_train = self.X_train.reshape((len(self.X_train), -1))
        self.X_test = self.X_test.reshape((len(self.X_test), -1))

        custom_pca = self.PrincipalComponentsAnalysis(n_components=self.n_components)
        custom_pca.fit(self.X_train)

        self.X_training_reduced = custom_pca.transform(self.X_train)
        self.X_test_reduced = custom_pca.transform(self.X_test)

    def predict(self):
        bestacc=0

        self.classifier = SVC(kernel='linear', C=self.C)
        self.classifier.fit(self.X_training_reduced, self.y_train)
        self.y_predicted = self.classifier.predict(self.X_test_reduced)
        self.accuracy = accuracy_score(self.y_actual, self.y_predicted)
        
    def show_results(self):
        print(self.accuracy)
        ConfusionMatrixDisplay.from_estimator(
            self.classifier, self.X_test_reduced, self.y_actual, xticks_rotation="vertical"
        )
        plt.tight_layout()
        plt.show()


class FisherFaces(FacialRecognitionAlgorithm):
    def __init__(self, modelname, dataset, num_features):
        super().__init__(modelname, dataset)
        
        self.num_features = min(num_features, min(dataset.images[0].shape))
        

    def normalize(self, matrix, mean_all):
        min_val = np.min(mean_all)
        max_val = np.max(mean_all)
        normalized_matrix = 255 * (matrix - min_val) / (max_val - min_val)
        return normalized_matrix

    def train(self):
        
        n = len(self.X_train)
        unique_labels = list(set(self.y_train))
        self.classes = unique_labels
        class_means = []

        for i in range(len(unique_labels)):
            label = unique_labels[i]
            matching_indexes = np.where(self.y_train == label)[0]
            class_samples = self.X_train[matching_indexes]  # each c
            class_mean = np.mean(class_samples, axis=0)
            class_means.append(class_mean)
            
        class_means = np.array(class_means)
        mean_all = np.mean(class_means, axis=0)

        X = np.array([image for image in self.X_train])

        Sw = np.zeros((self.num_features, self.num_features))  # within class
        Sb = np.zeros((self.num_features, self.num_features))  # between-class

        for i in range(len(unique_labels)):
            label = unique_labels[i]
            class_samples = X[self.y_train == label]  # each c

            for class_sample in class_samples:
                Sw += np.dot((class_sample - class_means[i]).T, (class_sample - class_means[i]))

            Sb += len(class_samples) * np.dot((class_means[i] - mean_all), (class_means[i] - mean_all).T)

        eigenvalues, eigenvectors = np.linalg.eig(np.dot(np.linalg.inv(Sw), (Sb)))
        sort = np.argsort(eigenvalues)[::-1]
        eigenvalues = eigenvalues[sort]
        eigenvectors = eigenvectors[:, sort]

        class_means = np.dot(class_means, eigenvectors)

        self.fisherfaces = eigenvectors
        self.mean_face = mean_all
        self.class_means = class_means
        self.basis = eigenvalues


    def predict(self):
        def predict1(image):
            projection = np.dot(image, self.fisherfaces)

            distances = []
            for c in self.class_means:
                distances.append(np.linalg.norm(projection - c))
            distances = np.array(distances)

            predicted_label = np.argmin(distances)

            distance = distances[predicted_label]
            return self.classes[predicted_label], distance
        self.accuracy = 0
        self.conf_matrix = np.zeros((len(self.dataset.numbers_to_labels), len(self.dataset.numbers_to_labels)))
        self.y_predicted=[]
        for x,y in zip(self.X_test, self.y_actual):
            
            pred = predict1(x)
            try:
                self.conf_matrix[self.dataset.labels_to_numbers[y], self.dataset.labels_to_numbers[pred[0]]] += 1
            except:
                self.conf_matrix[y, pred[0]] += 1
            self.y_predicted.append(pred[0])
            if y == pred[0]: self.accuracy += 1
        
        print('accuracy', self.accuracy/len(self.X_test))
        
    def show_results(self):
        conf_matrix = confusion_matrix(self.y_actual, np.asarray(self.y_predicted), labels=self.classes)
        
        plt.imshow(conf_matrix, interpolation='nearest')
        plt.title('Confusion Matrix')
        plt.colorbar()

        tick_marks = np.arange(len(self.classes))
        plt.xticks(tick_marks, self.classes, rotation=45)
        plt.yticks(tick_marks, self.classes)

        for i in range(conf_matrix.shape[0]):
            for j in range(conf_matrix.shape[1]):
                plt.text(j, i, format(conf_matrix[i, j],  'd'),
                        ha="center", va="center",
                        color="black")

        plt.ylabel('True label')
        plt.xlabel('Predicted label')
        plt.tight_layout()
        plt.show()

class CNN(FacialRecognitionAlgorithm):
    def __init__(self, modelname, dataset, epochs = 100):
        super().__init__(modelname, dataset)
        self.epochs=epochs
        print('init', self.epochs)
        
    def train(self):
    
        #train_images, val_images, train_labels, val_labels = train_test_split(images, labels, test_size=0.2, random_state=42)
        datagen = keras.preprocessing.image.ImageDataGenerator(
            rotation_range=20,
            horizontal_flip=True,
        )

        datagen.fit(self.X_train)

        self.train_data_generator = datagen.flow(self.X_train, self.y_train, batch_size=64, shuffle=True)
        self.test_data_generator = datagen.flow(self.X_test, self.y_actual, batch_size=64, shuffle=True)

        # display first image in val data batch and its label -- just for validation
        first_batch = next(self.test_data_generator)
        #display(keras.preprocessing.image.array_to_img(first_batch[0][0] * 255))
        print(first_batch[1][0])

        self.test_data_generator = datagen.flow(self.X_test, self.y_actual, batch_size=64, shuffle=True) # reset val_data_generator

        # Xception model from https://keras.io/examples/vision/image_classification_from_scratch/#using-image-data-augmentation

        # def make_model(input_shape, num_classes):
        #     inputs = keras.Input(shape=input_shape)

        #     # Entry block
        #     x = layers.Rescaling(1.0 / 255)(inputs)
        #     x = layers.Conv2D(128, 3, strides=2, padding="same")(x)
        #     x = layers.BatchNormalization()(x)
        #     x = layers.Activation("relu")(x)

        #     previous_block_activation = x  # Set aside residual

        #     for size in [256, 512, 728]:
        #         x = layers.Activation("relu")(x)
        #         x = layers.SeparableConv2D(size, 3, padding="same")(x)
        #         x = layers.BatchNormalization()(x)

        #         x = layers.Activation("relu")(x)
        #         x = layers.SeparableConv2D(size, 3, padding="same")(x)
        #         x = layers.BatchNormalization()(x)

        #         x = layers.MaxPooling2D(3, strides=2, padding="same")(x)

        #         # Project residual
        #         residual = layers.Conv2D(size, 1, strides=2, padding="same")(
        #             previous_block_activation
        #         )
        #         x = layers.add([x, residual])  # Add back residual
        #         previous_block_activation = x  # Set aside next residual

        #     x = layers.SeparableConv2D(1024, 3, padding="same")(x)
        #     x = layers.BatchNormalization()(x)
        #     x = layers.Activation("relu")(x)

        #     x = layers.GlobalAveragePooling2D()(x)
        #     if num_classes == 2:
        #         activation = "sigmoid"
        #         units = 1
        #     else:
        #         activation = "softmax"
        #         units = num_classes

        #     x = layers.Dropout(0.5)(x)
        #     outputs = layers.Dense(units, activation=activation)(x)
        #     return keras.Model(inputs, outputs)


        # model = make_model(input_shape=IMAGE_SIZE + (1, ), num_classes=len(frequencies))

        input_shape = self.dataset.images[0].shape + (1, )
        self.num_classes = len(set(self.dataset.labels_to_numbers))

        self.model = keras.Sequential(
            [
                keras.Input(shape=input_shape),
                layers.Conv2D(32, kernel_size=(3, 3), activation="relu"),
                layers.MaxPooling2D(pool_size=(2, 2)),
                layers.Conv2D(64, kernel_size=(3, 3), activation="relu"),
                layers.MaxPooling2D(pool_size=(2, 2)),
                layers.Flatten(),
                layers.Dropout(0.5),
                layers.Dense(self.num_classes, activation="softmax"),
            ]
        )

        next(self.train_data_generator)[1].shape
        self.callbacks = [
            keras.callbacks.ModelCheckpoint("save_at_{epoch}.keras"),
        ]
        self.model.compile(
            optimizer=keras.optimizers.legacy.Adam(1e-3), # legacy for m1 support
            loss="categorical_crossentropy",
                metrics=["accuracy"],
    )
        

    def predict(self):
        self.model.fit(
            self.train_data_generator,
            epochs=self.epochs,
            callbacks=self.callbacks,
            validation_data=self.test_data_generator,
        )

    
    def show_results(self):
        y_predicted = self.model.predict(self.X_test)
        
        # Convert predictions to class labels
        y_predicted = y_predicted.argmax(axis=1)
        y_predicted = np.asarray([self.dataset.numbers_to_labels[y_pred] for y_pred in y_predicted])
        
        self.y_actual = [self.dataset.numbers_to_labels[np.where(row == 1)[0][0]] for row in self.y_actual]
        

        self.accuracy = accuracy_score(self.y_actual, y_predicted)
        print("Final Validation Accuracy: {:.2%}".format(self.accuracy))

        # Display confusion matrix
        cm = confusion_matrix(self.y_actual, y_predicted)
        disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=sorted(self.dataset.labels_to_numbers.keys(), key=lambda x: self.dataset.labels_to_numbers[x]))
        disp.plot(cmap='viridis', values_format='d', xticks_rotation="vertical")

        plt.tight_layout()
        plt.show()


        
     
class LBPH(FacialRecognitionAlgorithm):
    def __init__(self, modelname, dataset,num_features, metric, radius=1):
        super().__init__(modelname,dataset)
        self.size = num_features
        self.metric = metric
        self.radius=radius

    def train(self):
        self.classes = list(set(self.y_train))
        histograms = {}
        for i, image in enumerate(self.X_train):
            histograms.setdefault(self.y_train[i], [])
            histograms[self.y_train[i]].append(self.get_Histogram(image))
        self.histograms = histograms
    
    def get_Histogram(self, grayscaled_mat): 
        lbp_mat = self.get_LBP_Mat(grayscaled_mat)
        #plt.imsave('altered_images/lbp'+name+".jpeg", lbp_mat)
        lbp_values = lbp_mat.flatten()
        histogram = np.histogram(lbp_values, bins=256, range=(0, 256))
        # if show_hist:
        #     self.show_histogram(histogram, name)
        return histogram[0]
    
    def get_LBP_Mat(self, grayscaled_mat):
        M,N=grayscaled_mat.shape
        lbp_matrix = np.zeros((M, N), dtype=np.uint8)
        for r in range(1,M-1):
            for c in range(1,N-1):
                pixel_threshold = grayscaled_mat[r][c]
               # neighbors = self.getNeighbors(grayscaled_mat,r,c)
                neighbors = self.getRNeighbors(grayscaled_mat, r, c)
                above_thresh = neighbors >= pixel_threshold
                lbp_rc = np.sum(above_thresh * (2 ** np.arange(len(neighbors))))
                lbp_matrix[r, c] = lbp_rc
        return lbp_matrix


    def distance(self,vector1, vector2, metric):
        
        if metric == "ChiSquare":
            distance = np.sum((vector1 - vector2)**2 / (vector1 + vector2 + 1e-10))

        elif metric == "EuclideanDistance":
            distance = np.linalg.norm(vector1 - vector2)

        elif metric == "NormalizedEuclideanDistance":
            distance = np.linalg.norm(vector1 - vector2) / np.linalg.norm(vector1)

        elif metric == "AbsoluteValue":
            distance = np.sum(np.abs(vector1 - vector2))

        else:
            raise ValueError("Invalid metric. Supported metrics: 'ChiSquare', 'EuclideanDistance', 'NormalizedEuclideanDistance', 'AbsoluteValue'.")

        return distance

    def getRNeighbors(self, mat, r, c):
        arr = []
        for i in range(r - self.radius, r + self.radius + 1):
            if i not in range(self.size): continue
            for j in range(c - self.radius, c + self.radius + 1):
                if j not in range(self.size): continue
                arr.append(mat[i][j])
        return np.array(arr)

    def predict(self):
        def predict1(image):
            
            myhist=self.get_Histogram(image)
            distances = []
            ppl=[]
            for person in self.histograms:
                for histogram in self.histograms[person]:
                    distances.append(self.distance(histogram,myhist,self.metric))
                    ppl.append(person)
            distances = np.array(distances)

            ind = np.argmin(distances)

            pred_label, distance= ppl[ind], distances[ind]

            return pred_label, distance
        self.accuracy = 0
        self.conf_matrix = np.zeros((len(self.dataset.numbers_to_labels), len(self.dataset.numbers_to_labels)))
        self.y_predicted=[]
        for x,y in zip(self.X_test, self.y_actual):
            
            pred = predict1(x)
            try:
                self.conf_matrix[self.dataset.labels_to_numbers[y], self.dataset.labels_to_numbers[pred[0]]] += 1
            except:
                self.conf_matrix[y, pred[0]] += 1
            self.y_predicted.append(pred[0])
            if y == pred[0]: self.accuracy += 1
    def show_results(self):
        print('accuracy', self.accuracy/len(self.X_test))
        conf_matrix = confusion_matrix(self.y_actual, np.asarray(self.y_predicted), labels=self.classes)
        
        plt.imshow(conf_matrix, interpolation='nearest')
        plt.title('Confusion Matrix')
        plt.colorbar()

        tick_marks = np.arange(len(self.classes))
        plt.xticks(tick_marks, self.classes, rotation=45)
        plt.yticks(tick_marks, self.classes)

        for i in range(conf_matrix.shape[0]):
            for j in range(conf_matrix.shape[1]):
                plt.text(j, i, format(conf_matrix[i, j],  'd'),
                        ha="center", va="center",
                        color="black")

        plt.ylabel('True label')
        plt.xlabel('Predicted label')
        plt.tight_layout()
        plt.show()


class DatasetLoader:

    #CREATED IN LOAD DATA : self.images, self.indices, self.labelnames, self.labeldict,self.reversedict, self.labels, self.target_names
    #CREATED IN SPLIT DATA: self.X_train, self.X_test, self.y_train, self.y_actual

    def load_data(self):
        # Implement the logic to load data from a specific dataset
        pass
        
    def split_data(self, prop):
        np.random.seed(18)
        np.random.shuffle(self.indices)

        cut = int((1 - prop) * len(self.images))//1

        X_train =  self.images[self.indices[:cut]]
        X_test = self.images[self.indices[cut:]]

        y_train = self.labelnames[self.indices[:cut]]
        y_actual = self.labelnames[self.indices[cut:]]
        
        return X_train, X_test, y_train, y_actual
    
class DatasetLoader:

    #CREATED IN LOAD DATA : self.images, self.indices, self.labelnames, self.labeldict,self.reversedict, self.labels, self.target_names
    #CREATED IN SPLIT DATA: self.X_train, self.X_test, self.y_train, self.y_actual

    def load_data(self):
        # Implement the logic to load data from a specific dataset
        pass
        
    def split_data(self, prop):
        np.random.seed(18)
        np.random.shuffle(self.indices)

        cut = int((1 - prop) * len(self.images))//1

        X_train =  self.images[self.indices[:cut]]
        X_test = self.images[self.indices[cut:]]

        y_train = self.labelnames[self.indices[:cut]]
        y_actual = self.labelnames[self.indices[cut:]]
        
        return X_train, X_test, y_train, y_actual

class BFW_Probabilistic(DatasetLoader):
    def load_data(self, modeltype, num_features=100, prop=0.2, probabilities=None):
        names = []
        images = []
        demographics = []
        random.seed(18)
        categories = [folder for folder in os.scandir('bfw') if folder.name != '.DS_Store']
        if probabilities is None:
            probabilities = {}

        for category in categories:
            people = [folder for folder in os.scandir(category.path) if folder.name != '.DS_Store']
            demographic = category.name
            demographic_probability = probabilities[demographic] if demographic in probabilities else 1
            for person in people[:1]:
                name = person.name
                person_images = [file for file in os.scandir(person.path) if file.name != '.DS_Store']
                if random.random() > demographic_probability:
                    continue
                for image in person_images:
                    images.append((image, num_features, num_features))
                    names.append(name)
                    demographics.append(demographic)
        
        unique_labels = list(set(names))
        self.images = np.array(images)
        self.labelnames = np.array(names)
        self.indices = np.arange(len(self.images))
        self.labels_to_numbers = {name: i for i, name in enumerate(set(names))}
        self.numbers_to_labels = {i: name for name, i in self.labels_to_numbers.items()}
        
        if modeltype=="CNN":
            images = np.expand_dims(np.asarray(self.images), axis=-1) # necessary to show there is 1 channel (grayscale)?
            labels = keras.utils.to_categorical(np.asarray([self.labels_to_numbers[label] for label in self.labelnames]))
            self.X_train, self.X_test, self.y_train, self.y_actual = train_test_split(images, labels, test_size=prop, random_state=18)
       
        else:
            self.X_train, self.X_test, self.y_train, self.y_actual = self.split_data(prop)
        
        return self
        
        
class BFW_Balanced(BFW_Probabilistic):
    def load_data(self, modeltype, num_features=100, prop=0.2 ):
        super().load_data(modeltype, num_features, prop, {})
        
        # self.images = s.images
        # self.labelnames = s.labelnames
        # self.indices = s.indices
        # self.labels_to_numbers = s.labels_to_numbers
        # self.numbers_to_labels = s.numbers_to_labels
        # self.X_train, self.X_test, self.y_train, self.y_actual = s.X_train, s.X_test, s.y_train, s.y_actual
    def split_data(self, prop=0.25):
        return super().split_data(prop)
class LFW(DatasetLoader):

        def load_data(self, modeltype, num_features=100, prop = .25, probabilities=None):
        
            face_dataset = datasets.fetch_lfw_people(min_faces_per_person=50)
            self.target_names = face_dataset.target_names
            if modeltype=='Fisher':
                num_features = min(num_features, min(face_dataset.images[0].shape))
            if modeltype!='CNN':
                face_dataset.images = [cv2.resize(image, (num_features, num_features)) for image in face_dataset.images]
            
            self.num_features = num_features
            kept_images = []
            kept_labels = []
            frequencies = {}

            for image, label in zip(face_dataset.images, face_dataset.target):
                if label not in frequencies:
                    frequencies[label] = 0
                if frequencies[label] > 150:
                    continue
                kept_images.append(image)
                kept_labels.append(label)
                frequencies[label] += 1

            if modeltype!='CNN':
                kept_images = np.array([vector.reshape((num_features, num_features)) for vector in kept_images])
            self.labels = np.asarray(kept_labels)

            self.numbers_to_labels = {i: name for i, name in enumerate(face_dataset.target_names)}
            self.labels_to_numbers = {name: i for i, name in enumerate(face_dataset.target_names)}
            
            labelnames = []
            for i in range(len(self.labels)):
                labelnames.append(self.numbers_to_labels[self.labels[i]])
            self.labelnames = np.asarray(labelnames)

            self.indices = np.arange(len(kept_images))
            self.images=np.asarray(kept_images)
            
            if modeltype=="CNN":
                
                self.X_train, self.X_test, self.y_train, self.y_actual = self.split_data()
                self.X_train = np.expand_dims(np.asarray(self.X_train), axis=-1) # necessary to show there is 1 channel (grayscale)?
                self.X_test = np.expand_dims(np.asarray(self.X_test), axis=-1) # necessary to show there is 1 channel (grayscale)?
                self.y_train = [self.labels_to_numbers[item] for item in self.y_train]
                self.y_actual = [self.labels_to_numbers[item] for item in self.y_actual]
                
                self.y_train = keras.utils.to_categorical(np.asarray(self.y_train))
                self.y_actual = keras.utils.to_categorical(np.asarray(self.y_actual))

                #print(images.shape)
        
            else:
                print(self.images.shape)
                self.X_train, self.X_test, self.y_train, self.y_actual = self.split_data()


        def split_data(self, prop=.2):
            return super().split_data(prop)
class TA_Data(DatasetLoader):

    def load_data(self, modeltype, num_features, prop, probabilities = None):
        self.images = []
        self.labelnames = []
        for filename in os.listdir('./ta_data'):
            # read in file from its filepath
            image = Image.open('./ta_data/' + filename).resize((num_features,num_features))
            # store name of person in file
            name = filename[0:filename.index('_')]
            # convert RGB pixel data into a list
            pixels = list(image.getdata())
            greyscale = np.zeros(num_features*num_features)
            for i in range(len(pixels)):
                rgb = pixels[i]
                # use weighted grayscale function to optimize lighting differences among color hues
                greyscale[i] = 0.299*rgb[0] + 0.587*rgb[1] + 0.114*rgb[2] 
            # reshape greyscale vector back into matrix form       
            greyscale = greyscale.reshape((num_features,num_features)) 

            # add greyscale image and label name for image to the 2 respective lists          
            self.images.append(greyscale)
            self.labelnames.append(name)
        
        # turn them into numpy arrays for proper indexing
        self.images = np.array(self.images)
        self.labelnames = np.array(self.labelnames)
        self.labels_to_numbers = {}
        i = 0
        for label in self.labelnames:
            if label in self.labels_to_numbers.keys():
                continue
            self.labels_to_numbers[label] = i
            i += 1

        self.numbers_to_labels = {value:key for key, value in self.labels_to_numbers.items()}
        
        # CNN Data Handling
        if modeltype=="CNN":


            images = np.expand_dims(np.asarray(self.images), axis=-1) # necessary to show there is 1 channel (grayscale)?
            labels = keras.utils.to_categorical(np.asarray([self.labels_to_numbers[label] for label in self.labelnames]))
            self.X_train, self.X_test, self.y_train, self.y_actual = train_test_split(images, labels, test_size=prop, random_state=18)
        # Eigenfaces, Fisherfaces, LBPH Data Handling
        else:
            self.indices = np.arange(len(self.images))
            self.X_train, self.X_test, self.y_train, self.y_actual = self.split_data(prop)

    def split_data(self, prop=.25):
        return super().split_data(prop)


class Yale_Data(DatasetLoader):
    def load_data(self, modeltype, num_features, prop, probabilities = None):
        images = []
        labels = []
        for filename in os.listdir('yalefaces_binary'):
            if filename == 'data' or filename == 'Readme.txt': continue
            with open('yalefaces_binary/' + filename, 'rb') as file:
            # Read the content of the binary file
                binary_data = file.read()
                image = Image.open(io.BytesIO(binary_data))
                resized_image = image.resize((num_features,num_features))
                images.append(np.array(resized_image))
                labels.append(filename[:filename.find('.')])
        
        self.images = np.array(images)
        self.labelnames = np.array(labels)

        self.labels_to_numbers = {}
        i = 0
        for label in self.labelnames:
            if label in self.labels_to_numbers.keys():
                continue
            self.labels_to_numbers[label] = i
            i += 1

        self.numbers_to_labels = {value:key for key, value in self.labels_to_numbers.items()}
        if modeltype=="CNN":

            images = np.expand_dims(np.asarray(self.images), axis=-1) # necessary to show there is 1 channel (grayscale)?
            labels = keras.utils.to_categorical(np.asarray([self.labels_to_numbers[label] for label in self.labelnames]))
            self.X_train, self.X_test, self.y_train, self.y_actual = train_test_split(images, labels, test_size=prop, random_state=18)
        else:
            self.indices = np.arange(len(self.images))
            self.X_train, self.X_test, self.y_train, self.y_actual = self.split_data(prop)
    
    def split_data(self, prop=.2):
        return super().split_data(prop)


class MLApp(QWidget):
    def __init__(self):
        super().__init__()
        self.init_ui()

    def init_ui(self):
        self.setWindowTitle('ML Algorithm Selector')
        self.setGeometry(100, 100, 600, 300)

        algolabel = QLabel('Select your ML algorithm:')
        self.algorithm_combo = QComboBox()
        self.algorithm_combo.addItems(['', 'Eigenfaces', 'Fisherfaces', 'LBPH', 'CNN'])
        self.algorithm_combo.currentIndexChanged.connect(self.update_hyperparameters)

        datalabel = QLabel('Select your Dataset:')
        self.dataset_combo = QComboBox()
        self.dataset_combo.addItems(['', 'LFW', 'TA Data', 'Yale Data', 'BFW Balanced', 'BFW Probabilistic'])
        self.dataset_combo.currentIndexChanged.connect(self.update_hyperparameters)
        self.hyperparam_layout = QFormLayout()

        self.num_features_widget = QLineEdit(str(150))
        self.train_test_split_widget = QLineEdit(str(0.8))
        self.additional_widget = QLineEdit("")
        self.num_features_label = QLabel("Number of Features:")
        self.train_test_split_label = QLabel("Train/Test Split Proportion:")
        self.additional_label = QLabel("")
        self.num_features_widget.setVisible(False)
        self.train_test_split_widget.setVisible(False)
        self.additional_widget.setVisible(False)
        self.num_features_label.setVisible(False)
        self.train_test_split_label.setVisible(False)
        self.additional_label.setVisible(False)

        self.hyperparam_layout.addRow(self.num_features_label, self.num_features_widget)
        self.hyperparam_layout.addRow(self.train_test_split_label, self.train_test_split_widget)
        self.hyperparam_layout.addRow(self.additional_label, self.additional_widget,)
        
        # self.c_widget = QLineEdit(str(1.0))
        # self.metric_widget = QLineEdit("ChiSquare")
        # self.epochs_widget = QLineEdit(str(75))

        self.men_subset_label = QLabel("Male-Identifying %:")
        self.women_subset_label = QLabel("Female-Identifying %:")
        self.caucasian_subset_label = QLabel("Caucasian-Identifying %:")
        self.black_subset_label = QLabel("Black-Identifying %:")
        self.indian_subset_label = QLabel("Indian-Identifying %:")
        self.asian_subset_label = QLabel("Asian-Identifying %:")
        self.men_subset_widget = QLineEdit()
        self.women_subset_widget = QLineEdit()
        self.caucasian_subset_widget = QLineEdit()
        self.black_subset_widget = QLineEdit()
        self.indian_subset_widget = QLineEdit()
        self.asian_subset_widget = QLineEdit()
        self.demographic_widgets = [self.men_subset_label,self.women_subset_label,self.caucasian_subset_label,self.black_subset_label,self.asian_subset_label,self.indian_subset_label,self.men_subset_widget,self.women_subset_widget,self.caucasian_subset_widget,self.black_subset_widget, self.indian_subset_widget, self.asian_subset_widget]
        self.demographic_layout = QGridLayout()
        self.demographic_layout.addWidget(self.men_subset_label, 0, 0)
        self.demographic_layout.addWidget(self.men_subset_widget, 0, 1)
        self.demographic_layout.addWidget(self.women_subset_label, 0, 2)
        self.demographic_layout.addWidget(self.women_subset_widget, 0, 3)

        self.demographic_layout.addWidget(self.asian_subset_label, 2, 0)
        self.demographic_layout.addWidget(self.asian_subset_widget, 2, 1)
        self.demographic_layout.addWidget(self.black_subset_label, 2, 2)
        self.demographic_layout.addWidget(self.black_subset_widget, 2, 3)

        self.demographic_layout.addWidget(self.caucasian_subset_label, 3, 0)
        self.demographic_layout.addWidget(self.caucasian_subset_widget, 3, 1)
        self.demographic_layout.addWidget(self.indian_subset_label, 3, 2)
        self.demographic_layout.addWidget(self.indian_subset_widget, 3, 3)

        for widget in self.demographic_widgets:
            widget.setVisible(False)

        train_test_button = QPushButton('Train/Test Algorithm')
        train_test_button.clicked.connect(self.train_test_algorithm)

        layout = QVBoxLayout()
        layout.addWidget(algolabel)
        layout.addWidget(self.algorithm_combo)
        layout.addWidget(datalabel)
        layout.addWidget(self.dataset_combo)
        layout.addLayout(self.demographic_layout)
        layout.addLayout(self.hyperparam_layout)
        layout.addWidget(train_test_button)

        self.setLayout(layout)
        self.show()

    def update_hyperparameters(self):
        if self.algorithm_combo.currentText() == "":
            self.num_features_widget.setVisible(False)
            self.train_test_split_widget.setVisible(False)
            self.additional_widget.setVisible(False)
            self.num_features_label.setVisible(False)
            self.train_test_split_label.setVisible(False)
            self.additional_label.setVisible(False)
        else:
            self.num_features_widget.setVisible(True)
            self.train_test_split_widget.setVisible(True)
            self.additional_widget.setVisible(True)
            self.num_features_label.setVisible(True)
            self.train_test_split_label.setVisible(True)
            self.additional_label.setVisible(True)
            self.num_features_widget.setText(str(150))
            if self.algorithm_combo.currentText() == "Eigenfaces":
                self.additional_widget.setText(str(1.0))
                self.additional_label.setText("C Value:")
            if self.algorithm_combo.currentText() == "Fisherfaces":
                self.num_features_widget.setText(str(40))
                self.additional_widget.setVisible(False)
                self.additional_label.setVisible(False)
            if self.algorithm_combo.currentText() == "LBPH":
                self.additional_widget.setText("ChiSquare")
                self.additional_label.setText("Distance Metric:")
            if self.algorithm_combo.currentText() == "CNN":
                self.additional_widget.setText(str(75))
                self.additional_label.setText("Number of Epochs:")
        if self.dataset_combo.currentText()=="BFW Probabilistic":
            for widget in self.demographic_widgets:
                widget.setVisible(True)
        else:
            for widget in self.demographic_widgets:
                widget.setVisible(False)
        

    def validate_proportions(self):
        men_women_proportion = sum(float(widget.text()) for widget in self.demographic_widgets[6:8] if widget.text())
        others_proportion = sum(float(widget.text()) for widget in self.demographic_widgets[8:]if widget.text())
        if men_women_proportion != 100.0 :
            QMessageBox.warning(self, "Invalid Proportions", "Men and Women must add up to 100.")
            return False
        if others_proportion != 100.0:
            QMessageBox.warning(self, "Invalid Proportions", "Racial Groups must add up to 100.")
            return False
        # self.probabilities = {
        #     "Male": float(self.demographic_widgets[6].currentText()),
        #     "Female": float(self.demographic_widgets[7].currentText()),
        #     "White": float(self.demographic_widgets[8].currentText()),
        #     "Black": float(self.demographic_widgets[9].currentText()),
        #     "Indian": float(self.demographic_widgets[10].currentText()),
        #     "Asian": float(self.demographic_widgets[11].currentText())
        # }
        self.probabilities = {
            "Male": float(self.demographic_widgets[6].text()),
            "Female": float(self.demographic_widgets[7].text()),
            "White": float(self.demographic_widgets[8].text()),
            "Black": float(self.demographic_widgets[9].text()),
            "Indian": float(self.demographic_widgets[10].text()),
            "Asian": float(self.demographic_widgets[11].text())
        }
    

        return True


    def train_test_algorithm(self):
        
        datasettype = self.dataset_combo.currentText()
        modeltype = self.algorithm_combo.currentText()
        num_features = int(self.num_features_widget.text())
        train_test_split = float(self.train_test_split_widget.text())
        self.probabilities={}
        if datasettype == "LFW":
            dataset = LFW()
        elif datasettype=="":
            return
        elif datasettype == "TA Data":
            dataset = TA_Data()
        elif datasettype == "Yale Data":
            dataset = Yale_Data()
        elif datasettype == "BFW Probabilistic":
            if not self.validate_proportions():
                return
            dataset = BFW_Probabilistic()
        elif datasettype == "BFW Balanced":
            dataset = BFW_Balanced()

        dataset.load_data(modeltype, num_features=num_features, prop=1 - train_test_split, probabilities = self.probabilities)

        if modeltype == 'Eigenfaces':
            print("Running Eigenfaces")
            c_value = float(self.additional_widget.text())
            f = Eigenfaces(modeltype, dataset, num_features,c_value=1)
        elif modeltype == 'Fisherfaces':
            print("Running Fisherfaces")
            f = FisherFaces(modeltype, dataset, num_features)
        elif modeltype == 'LBPH':
            print("Running LBPH")
            metric = self.additional_widget.text()
            f = LBPH(modeltype, dataset, num_features, metric)
        elif modeltype == 'CNN':
            print("RUNNING CNN")
            epochs = int(self.additional_widget.text())
            print('running', epochs)
            f = CNN(modeltype, dataset, epochs)

            # Print all hyperparameter values and the dataset being used on separate lines
        print(f"Number of Features: {num_features}")
        print(f"Train/Test Split Proportion: {train_test_split}")
        print(f"Dataset being used: {datasettype}")
        if modeltype == 'Eigenfaces':
            print(f"C Value: {c_value}")
        if modeltype == "LBPH":
            print(f"Distance Metric: {metric}")
        if modeltype == "CNN":
            print(f"# of Epochs: {epochs}")
        f.X_train, f.X_test, f.y_train, f.y_actual = f.dataset.X_train, f.dataset.X_test, f.dataset.y_train, f.dataset.y_actual
        f.train()
        f.predict()
        f.show_results()
if __name__ == '__main__':
 
    app = QApplication(sys.argv)
    ml_app = MLApp()
    ml_app.show()
    sys.exit(app.exec_())

